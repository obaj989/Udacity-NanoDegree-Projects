{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "--describe your project at a high level--\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Importing everything required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from configparser import ConfigParser\n",
    "from os import environ\n",
    "\n",
    "from pandas import to_timedelta, Timestamp, read_csv\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql.functions import udf, col, lit, year, month, upper, to_date, monotonically_increasing_id, count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Setting up the configurations from the config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "config = ConfigParser()\n",
    "config.read('AWS.cfg', encoding='utf-8-sig')\n",
    "\n",
    "environ['AWS_ACCESS_KEY_ID'] = config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "environ['AWS_SECRET_ACCESS_KEY'] = config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "output_data_path = config['S3']['DEST_S3_BUCKET']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Creating a Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\") \\\n",
    "    .config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11,org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "    .enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Creating a user defined function to parse the date in correct format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "convert_to_datetime = udf(lambda date: to_timedelta(date, unit='D') + Timestamp('1960-1-1') if date else None, DateType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## US National Tourism and Trade Office Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Reading the data in spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immigration_data_path = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "df = spark.read.format('com.github.saurfang.sas.spark').load(immigration_data_path)\n",
    "df.createOrReplaceTempView('immigration_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### IMMIGRATION_INFO Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# extracting the required columns\n",
    "\n",
    "immigration_info = spark.sql(\"\"\"\n",
    "        SELECT DISTINCT cicid AS cic_id,\n",
    "                        i94yr AS year, \n",
    "                        i94mon  AS month, \n",
    "                        i94port AS city_code,\n",
    "                        i94addr AS state_code,\n",
    "                        arrdate AS arrive_date, \n",
    "                        depdate  AS departure_date, \n",
    "                        i94mode AS mode,\n",
    "                        i94visa AS visa\n",
    "        FROM immigration_data\n",
    "    \"\"\")\n",
    "# add the primary key\n",
    "immigration_info = immigration_info.withColumn(\"immigration_id\", monotonically_increasing_id())\n",
    "\n",
    "# adding country info for readability\n",
    "immigration_info = immigration_info.withColumn('country', lit('United States'))\n",
    "\n",
    "# changing the date type columns to have datetime values\n",
    "immigration_info = immigration_info.withColumn('arrive_date', convert_to_datetime(col('arrive_date')))\n",
    "immigration_info = immigration_info.withColumn('departure_date', convert_to_datetime(col('departure_date')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### IMMIGRATION_PERSONAL Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# extracting the required columns\n",
    "immigration_personal = spark.sql(\"\"\"\n",
    "            SELECT  DISTINCT cicid AS cic_id, \n",
    "                             i94cit AS citizen_country, \n",
    "                             i94res  AS residence_country, \n",
    "                             biryear AS birth_year,\n",
    "                             gender,\n",
    "                             insnum AS ins_num\n",
    "            FROM immigration_data\n",
    "        \"\"\")\n",
    "# add the primary key\n",
    "immigration_personal = immigration_personal.withColumn(\"personal_id\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### IMMIGRATION_AIRLINE Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# extracting the required columns\n",
    "\n",
    "immigration_airline = spark.sql(\"\"\"\n",
    "                SELECT  DISTINCT cicid AS cic_id, \n",
    "                                 airline, \n",
    "                                 admnum  AS admin_num, \n",
    "                                 fltno AS flight_number,\n",
    "                                 visatype AS visa_type\n",
    "                FROM immigration_data\n",
    "            \"\"\")\n",
    "# add the primary key\n",
    "immigration_airline = immigration_airline.withColumn(\"airline_id\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### I94 SAS labels desctiption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "We can examine the contents of the file by opening it in any editor. The magic numbers we see in the code have been obtained by carefull examination of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# reading the file\n",
    "\n",
    "with open(\"I94_SAS_Labels_Descriptions.SAS\") as f:\n",
    "    contents = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### COUNTRY_CODE Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Extracting country codes from labes data. The magic numbers here have ben chosen after inspecting the file manually\n",
    "country_code = {}\n",
    "for countries in contents[10:298]:\n",
    "    pair = countries.split('=')\n",
    "    code, country = pair[0].strip(), pair[1].strip().strip(\"'\")\n",
    "    country_code[code] = country\n",
    "df_country_codes = spark.createDataFrame(country_code.items(), ['code', 'country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(code='236', country='AFGHANISTAN'),\n",
       " Row(code='101', country='ALBANIA'),\n",
       " Row(code='316', country='ALGERIA'),\n",
       " Row(code='102', country='ANDORRA'),\n",
       " Row(code='324', country='ANGOLA')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_country_codes.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### CITY_CODE Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Extracting city codes from labes data. The magic numbers here have ben chosen after inspecting the file manually\n",
    "city_code = {}\n",
    "for cities in contents[303:962]:\n",
    "    pair = cities.split('=')\n",
    "    code, city = pair[0].strip(\"\\t\").strip().strip(\"'\"),  pair[1].strip('\\t').strip().strip(\"''\")\n",
    "    city_code[code] = city\n",
    "df_city_codes = spark.createDataFrame(city_code.items(), ['code', 'city'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|code|                city|\n",
      "+----+--------------------+\n",
      "| ANC|ANCHORAGE, AK    ...|\n",
      "| BAR|BAKER AAF - BAKER...|\n",
      "| DAC|DALTONS CACHE, AK...|\n",
      "| PIZ|DEW STATION PT LA...|\n",
      "| DTH|DUTCH HARBOR, AK ...|\n",
      "| EGL|EAGLE, AK        ...|\n",
      "| FRB|FAIRBANKS, AK    ...|\n",
      "| HOM|HOMER, AK        ...|\n",
      "| HYD|HYDER, AK        ...|\n",
      "| JUN|JUNEAU, AK       ...|\n",
      "| 5KE|       KETCHIKAN, AK|\n",
      "| KET|KETCHIKAN, AK    ...|\n",
      "| MOS|MOSES POINT INTER...|\n",
      "| NIK|NIKISKI, AK      ...|\n",
      "| NOM|NOM, AK          ...|\n",
      "| PKC|POKER CREEK, AK  ...|\n",
      "| ORI|  PORT LIONS SPB, AK|\n",
      "| SKA|SKAGWAY, AK      ...|\n",
      "| SNP| ST. PAUL ISLAND, AK|\n",
      "| TKI|          TOKEEN, AK|\n",
      "+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_city_codes.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### STATE_CODE Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Extracting state codes from labes data. The magic numbers here have ben chosen after inspecting the file manually\n",
    "state_code = {}\n",
    "for states in contents[982:1036]:\n",
    "    pair = states.split('=')\n",
    "    code, state = pair[0].strip('\\t').strip(\"'\"), pair[1].strip().strip(\"'\")\n",
    "    state_code[code] = state\n",
    "\n",
    "df_state_codes = spark.createDataFrame(state_code.items(), ['code', 'state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+\n",
      "|code|            state|\n",
      "+----+-----------------+\n",
      "|  AK|           ALASKA|\n",
      "|  AZ|          ARIZONA|\n",
      "|  AR|         ARKANSAS|\n",
      "|  CA|       CALIFORNIA|\n",
      "|  CO|         COLORADO|\n",
      "|  CT|      CONNECTICUT|\n",
      "|  DE|         DELAWARE|\n",
      "|  DC|DIST. OF COLUMBIA|\n",
      "|  FL|          FLORIDA|\n",
      "|  GA|          GEORGIA|\n",
      "|  GU|             GUAM|\n",
      "|  HI|           HAWAII|\n",
      "|  ID|            IDAHO|\n",
      "|  IL|         ILLINOIS|\n",
      "|  IN|          INDIANA|\n",
      "|  IA|             IOWA|\n",
      "|  KS|           KANSAS|\n",
      "|  KY|         KENTUCKY|\n",
      "|  LA|        LOUISIANA|\n",
      "|  ME|            MAINE|\n",
      "+----+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_state_codes.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## World Temperature Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# reading the data\n",
    "df = spark.read.csv('../../data2/GlobalLandTemperaturesByCity.csv', header=True)\n",
    "df.createOrReplaceTempView(\"temp_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### TEMPERATURE Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# extarcting the required columns\n",
    "temperature = spark.sql(\"\"\"\n",
    "    SELECT DISTINCT dt, \n",
    "                    AverageTemperature AS avg_temp, \n",
    "                    AverageTemperatureUncertainty  AS avg_temp_uncertainity, \n",
    "                    City AS city,\n",
    "                    Country AS country\n",
    "    FROM temp_data\n",
    "    WHERE country == 'United States'\n",
    "\"\"\")\n",
    "\n",
    "# add the primary key\n",
    "temperature = temperature.withColumn(\"temperature_id\", monotonically_increasing_id())\n",
    "# format the columns\n",
    "temperature = temperature.withColumn('dt', to_date(col('dt')))\n",
    "temperature = temperature.withColumn('year', year(temperature['dt']))\n",
    "temperature = temperature.withColumn('month', month(temperature['dt']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## US city demogrphics dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# reading the data\n",
    "\n",
    "df = spark.read.format('csv').options(header=True, delimiter=';').load('us-cities-demographics.csv')\n",
    "df.createOrReplaceTempView('demographics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### DEMOGRAPHICS_POPULATION Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# extarcting the required columns\n",
    "\n",
    "demographics = spark.sql(\"\"\"\n",
    "        SELECT DISTINCT UPPER(City) AS city, \n",
    "                        UPPER(State) AS state, \n",
    "                        \"Median Age\"  AS median_age, \n",
    "                        \"Average Household Size\" AS avg_household_size, \n",
    "                        \"Male Population\"  AS male_population, \n",
    "                        \"Female Population\" AS female_population,\n",
    "                        \"Number of Veterans\" AS num_vetarans,\n",
    "                        \"Foreign-born\" AS foreign_born,\n",
    "                        Race AS race\n",
    "        FROM demographics\n",
    "    \"\"\")\n",
    "# add the primary key\n",
    "demographics = demographics.withColumn(\"demographic_id\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Data Quality Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "We can perform null check to make sure that the data is correctly inserted into the tables and no tables are empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table: immigration_info\n",
      "Data quality check passed for immigration_info. Rows found: 3096313\n",
      "root\n",
      " |-- cic_id: double (nullable = true)\n",
      " |-- year: double (nullable = true)\n",
      " |-- month: double (nullable = true)\n",
      " |-- city_code: string (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- arrive_date: date (nullable = true)\n",
      " |-- departure_date: date (nullable = true)\n",
      " |-- mode: double (nullable = true)\n",
      " |-- visa: double (nullable = true)\n",
      " |-- immigration_id: long (nullable = false)\n",
      " |-- country: string (nullable = false)\n",
      "\n",
      "Table: immigration_personal\n",
      "Data quality check passed for immigration_personal. Rows found: 3096313\n",
      "root\n",
      " |-- cic_id: double (nullable = true)\n",
      " |-- citizen_country: double (nullable = true)\n",
      " |-- residence_country: double (nullable = true)\n",
      " |-- birth_year: double (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- ins_num: string (nullable = true)\n",
      " |-- personal_id: long (nullable = false)\n",
      "\n",
      "Table: immigration_airline\n",
      "Data quality check passed for immigration_airline. Rows found: 3096313\n",
      "root\n",
      " |-- cic_id: double (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admin_num: double (nullable = true)\n",
      " |-- flight_number: string (nullable = true)\n",
      " |-- visa_type: string (nullable = true)\n",
      " |-- airline_id: long (nullable = false)\n",
      "\n",
      "Table: temperature\n",
      "Data quality check passed for temperature. Rows found: 687004\n",
      "root\n",
      " |-- dt: date (nullable = true)\n",
      " |-- avg_temp: string (nullable = true)\n",
      " |-- avg_temp_uncertainity: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- temperature_id: long (nullable = false)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      "\n",
      "Table: demographics\n",
      "Data quality check passed for demographics. Rows found: 2891\n",
      "root\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- median_age: string (nullable = false)\n",
      " |-- avg_household_size: string (nullable = false)\n",
      " |-- male_population: string (nullable = false)\n",
      " |-- female_population: string (nullable = false)\n",
      " |-- num_vetarans: string (nullable = false)\n",
      " |-- foreign_born: string (nullable = false)\n",
      " |-- race: string (nullable = true)\n",
      " |-- demographic_id: long (nullable = false)\n",
      "\n",
      "Table: country_codes\n",
      "Data quality check passed for country_codes. Rows found: 288\n",
      "root\n",
      " |-- code: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      "\n",
      "Table: city_codes\n",
      "Data quality check passed for city_codes. Rows found: 659\n",
      "root\n",
      " |-- code: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n",
      "Table: state_codes\n",
      "Data quality check passed for state_codes. Rows found: 54\n",
      "root\n",
      " |-- code: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "strs = ['immigration_info', 'immigration_personal', 'immigration_airline',\n",
    "        'temperature', 'demographics', 'country_codes', 'city_codes', 'state_codes']\n",
    "tables = [immigration_info, immigration_personal, immigration_airline, temperature,\n",
    "          demographics, df_country_codes, df_city_codes, df_state_codes]\n",
    "\n",
    "for df, name in zip(tables, strs):\n",
    "    print(f'Table: {name}')\n",
    "    record_num = df.count()\n",
    "    if record_num == 0:\n",
    "        raise ValueError(f\"Records are not inserted properly. Table {name} is empty\")\n",
    "    else:\n",
    "        print(f'Data quality check passed for {name}. Rows found: {record_num}')\n",
    "    df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "We can perform integrity/type check to make sure that all the dates are in datetime format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integrity check passed for immigration_info table.\n"
     ]
    }
   ],
   "source": [
    "if type(immigration_info.schema['arrive_date'].dataType) == DateType and \\\n",
    "        type(immigration_info.schema['departure_date'].dataType) == DateType:\n",
    "\n",
    "    print(f'Integrity check passed for immigration_info table.')\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"Records are not inserted in the right format in immigration_info table.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Performing Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* How many people from a specific country visit the city of new york in a given year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "|code| country|\n",
      "+----+--------+\n",
      "| 258|PAKISTAN|\n",
      "+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_country_codes[df_country_codes['country'] == 'PAKISTAN'].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|code|                city|\n",
      "+----+--------------------+\n",
      "| NYC|NEW YORK, NY     ...|\n",
      "+----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_city_codes[df_city_codes['city'].contains('NEW YORK')].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "joined_immigration = immigration_info.join(immigration_personal, immigration_info.cic_id == immigration_personal.cic_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|  year|count|\n",
      "+------+-----+\n",
      "|2016.0| 1380|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_immigration.where(joined_immigration.city_code == 'NYC').where(joined_immigration.citizen_country == 258).groupby(\"year\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* How many people from a specific country have visited every city?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                city|count|\n",
      "+--------------------+-----+\n",
      "|FORT MYERS, FL   ...|   26|\n",
      "|LOS ANGELES, CA  ...|  343|\n",
      "|WEST PALM BEACH, ...|   46|\n",
      "|THOUSAND ISLAND B...|   14|\n",
      "|NEW YORK, NY     ...| 1380|\n",
      "|DETROIT, MI      ...|  142|\n",
      "|AUSTIN, TX       ...|    3|\n",
      "|PORT EVERGLADES, ...|    1|\n",
      "|SHANNON, IRELAND ...|    1|\n",
      "|FORT LAUDERDALE, ...|  110|\n",
      "|ORLANDO, FL      ...|   98|\n",
      "|  KAHULUI - MAUI, HI|   14|\n",
      "|DEL BONITA, MT   ...|    1|\n",
      "|OTTAWA, CANADA   ...|    8|\n",
      "|MONTREAL, CANADA ...|   12|\n",
      "|BRADENTON - SARAS...|    2|\n",
      "|ST PAUL, MN      ...|   18|\n",
      "|CALGARY, CANADA  ...|   14|\n",
      "|CHARLOTTE, NC    ...|   20|\n",
      "|  No PORT Code (YGF)|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_immigration.join(df_city_codes, joined_immigration.city_code == df_city_codes.code).where(joined_immigration.citizen_country == 258).groupby(\"city\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Writing Data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# writing immigration_info table to S3 in parquet files after partitioning by state\n",
    "immigration_info.write.mode(\"ignore\").partitionBy('state_code').parquet(output_data_path + 'immigration_info')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# writing immigration_personal table to S3 in parquet files\n",
    "immigration_personal.write.mode(\"ignore\").parquet(output_data_path + 'immigration_personal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# writing immigration_airline table to S3 in parquet files\n",
    "immigration_airline.write.mode(\"ignore\").parquet(output_data_path + 'immigration_airline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# write temperature table to parquet files\n",
    "temperature.write.mode(\"ignore\").parquet(output_data_path + 'temperature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# write demographics table to parquet files\n",
    "demographics.write.mode(\"ignore\").parquet(output_data_path + 'demographics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# writing state_codes table to S3 in parquet files\n",
    "df_state_codes.write.mode(\"ignore\").parquet(output_data_path + 'state_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# writing country_codes table to S3 in parquet files\n",
    "df_country_codes.write.mode(\"ignore\").parquet(output_data_path + 'country_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# writing city_codes table to S3 in parquet files\n",
    "df_city_codes.write.mode(\"ignore\").parquet(output_data_path + 'city_code')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
