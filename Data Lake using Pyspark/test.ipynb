{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format, monotonically_increasing_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('dl.cfg'))\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = config['AWS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "input_data = \"s3a://udacity-dend/\"\n",
    "output_data = \"s3a://udacity-dend-sparkproject4-osama/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "song_data = \"{}song_data/*/*/*/*.json\".format(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.json(song_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "songs_table = df.select(['song_id','title','artist_id','year','duration'])\n",
    "songs_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "s_path = f\"{output_data}/songs.parquet\"\n",
    "songs_table.write.partitionBy(['year','artist_id']).parquet(s_path, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "artists_table = df.selectExpr(\"artist_id\",\n",
    "                              \"artist_name as name\",\n",
    "                              \"artist_location as location\",\n",
    "                              \"artist_latitude as latitude\",\n",
    "                              \"artist_longitude as longitude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "a_path = f\"{output_data}/artists.parquet\"\n",
    "artists_table.write.parquet(a_path, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "log_data = \"{}log_data/*/*/*.json\".format(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_log_data = spark.read.json(log_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# filter by actions for song plays\n",
    "df_log_data = df_log_data.filter(df_log_data.page == 'NextSong')\n",
    "\n",
    "# extract columns for users table    \n",
    "users_table = df_log_data.selectExpr('userId as user_id','firstName as first_name','lastName as last_name','gender','level').distinct()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# write users table to parquet files\n",
    "users_table.write.parquet(\"{}/users.parquet\".format(output_data), mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create timestamp column from original timestamp column\n",
    "get_timestamp = udf(lambda ts: datetime.fromtimestamp(ts / 1000.0))\n",
    "df_log_data = df_log_data.withColumn(\"timestamp\", get_timestamp(\"ts\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create datetime column from original timestamp column\n",
    "get_datetime = udf(lambda ts: datetime.fromtimestamp(ts / 1000.0).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "df_log_data = df_log_data.withColumn(\"datetime\", get_datetime(\"ts\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_log_data.createOrReplaceTempView(\"logs\") \n",
    "\n",
    "time_table = spark.sql(\"\"\"\n",
    "    SELECT  DISTINCT datetime AS start_time, \n",
    "                     hour(timestamp) AS hour, \n",
    "                     day(timestamp)  AS day, \n",
    "                     weekofyear(timestamp) AS week,\n",
    "                     month(timestamp) AS month,\n",
    "                     year(timestamp) AS year,\n",
    "                     dayofweek(timestamp) AS weekday\n",
    "    FROM logs\n",
    "    ORDER BY start_time\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# write time table to parquet files partitioned by year and month\n",
    "time_table.write.partitionBy(['year','month']).parquet(\"{}/times.parquet\".format(output_data), mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# extract columns from joined song and log datasets to create songplays table\n",
    "df_joined = df_log_data.join(df, (df_log_data.artist == df.artist_name) & (df_log_data.song == df.title))\n",
    "df_joined = df_joined.withColumn(\"songplay_id\", monotonically_increasing_id())\n",
    "\n",
    "df_joined.createOrReplaceTempView(\"songplays\")\n",
    "songplays_table = spark.sql(\"\"\"\n",
    "    SELECT  songplay_id, \n",
    "            timestamp   AS start_time, \n",
    "            userId      AS user_id, \n",
    "            level,\n",
    "            song_id,\n",
    "            artist_id,\n",
    "            sessionId   AS session_id,\n",
    "            location,\n",
    "            userAgent   AS user_agent\n",
    "    FROM songplays\n",
    "    ORDER BY (user_id, session_id) \n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "songplays_table.write.parquet(\"{}/songplays.parquet\".format(output_data), mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT  songplay_id, \n",
    "            timestamp   AS start_time, \n",
    "            userId      AS user_id, \n",
    "            level,\n",
    "            song_id,\n",
    "            artist_id,\n",
    "            sessionId   AS session_id,\n",
    "            location,\n",
    "            userAgent   AS user_agent\n",
    "    FROM songplays\n",
    "    ORDER BY (user_id, session_id) \n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
